# Orquestador de GPUs con RunPod API

Este proyecto implementa un sistema automatizado para gestionar colas de generaci√≥n de im√°genes utilizando la infraestructura de RunPod (Pods Community Cloud).

## üìã Caracter√≠sticas
* **Auto-Scaling:** Levanta Pods GPU autom√°ticamente cuando hay trabajos en cola y los apaga al terminar para ahorrar costes.
* **Gesti√≥n de Colas:** Sistema FIFO con reintentos (backoff exponencial) y Dead Letter Queue (DLQ).
* **Control de Costes:** Monitorizaci√≥n de gasto por sesi√≥n y l√≠mites diarios configurables.
* **Seguridad:** Filtrado de prompts y validaci√≥n de inputs.

## üöÄ Instalaci√≥n
1. Clonar el repositorio.
2. Instalar dependencias:
   ```bash
   pip install runpod python-dotenv requests numpy

3. Configurar el archivo .env:

RUNPOD_API_KEY=tu_api_key_aqui
‚öôÔ∏è Uso
Para iniciar el orquestador (modo escucha):

Bash
python queue_system.py
El sistema comenzar√° a procesar los trabajos en cola.

Nota: Aseg√∫rate de ejecutar benchmark.py primero para ver qu√© modelo (nombre exacto del archivo .safetensors) tiene tu Pod, y actualiza el workflow_api.json si es necesario.

üèóÔ∏è Arquitectura
Se ha optado por el uso de Pods Persistentes (Community Cloud) en lugar de Serverless.

Motivo: Permite mantener el estado de ComfyUI (modelos cargados en VRAM) entre generaciones consecutivas, eliminando el tiempo de "cold-start" y reduciendo el coste efectivo para lotes grandes.

üõ†Ô∏è Troubleshooting
Error de conexi√≥n: Verificar que el Pod tiene el puerto 8188 expuesto p√∫blicamente.

Error "Node not found": Asegurarse de que el workflow_api.json solo usa nodos nativos o instalados en la imagen Docker.
